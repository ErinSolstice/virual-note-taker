{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from fpdf import FPDF\n",
    "from autocorrect import Speller\n",
    "import preprocessing\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find whiteboard\n"
     ]
    }
   ],
   "source": [
    "# Change img_name to image path\n",
    "img_path = \"sampleImages/f05-352z-02.jpg\"\n",
    "\n",
    "# removing file extension to get image_name\n",
    "img_name = Path(img_path).stem\n",
    "\n",
    "# read image into memory\n",
    "img = cv2.imread(img_path)\n",
    "# copy image\n",
    "img_copy = img.copy()\n",
    "# run preprocessing on the img\n",
    "prepro_img = preprocessing.preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display original and processed images\n",
    "cv2.imshow('img', img)\n",
    "cv2.imshow('prepro_img', prepro_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loads the readers for each model.\n",
    "# If using one of the models with a recog network specified you need to follow the steps for using a custom model in the README.md\n",
    "\n",
    "# base model\n",
    "# reader_base = easyocr.Reader(['en'])\n",
    "# model trained on IAM-onDB dataset\n",
    "reader_wb = easyocr.Reader(['en'], model_storage_directory='model', user_network_directory='user_network', recog_network='wb_v2')\n",
    "# further training of base model using IAM-onDB dataset and synthetic data generated with TextRecognitionDataGenerator\n",
    "# reader_g2_wb_bel = easyocr.Reader(['en'], model_storage_directory='model', user_network_directory='user_network', recog_network='g2_wb_bel_v2')\n",
    "\n",
    "\n",
    "# Loads the spell check\n",
    "#spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use OCR model to extract text\n",
    "# Uncomment the line with the reader and img you wish to use\n",
    "# original image\n",
    "#results = reader_base.readtext(img)\n",
    "results = reader_wb.readtext(img)\n",
    "#results = reader_g2_wb_bel.readtext(img)\n",
    "\n",
    "# preprocessed image\n",
    "#results = reader_base.readtext(prepro_img)\n",
    "#results = reader_wb.readtext(prepro_img)\n",
    "#results = reader_g2_wb_bel.readtext(prepro_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 0.2977: marlrt lar ca wlla lal.\n",
      "[INFO] 0.1284: eatbtratlebe.\n",
      "[INFO] 0.4127: p dee na.\n",
      " marlrt lar ca wlla lal. eatbtratlebe. p dee na.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup Function\n",
    "def cleanup_text(text):\n",
    "    # strip out non-ASCII text so we can draw the text on the image\n",
    "    # using OpenCV\n",
    "    return \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "\n",
    "# Duplicate image\n",
    "cloneImg = img.copy()\n",
    "\n",
    "# loop over the results and clean up text\n",
    "for (bbox, text, prob) in results:\n",
    "    # display the OCR text and associated probability\n",
    "    print(\"[INFO] {:.4f}: {}\".format(prob, text))\n",
    "    # unpack the bounding box\n",
    "    (tl, tr, br, bl) = bbox\n",
    "    tl = (int(tl[0]), int(tl[1]))\n",
    "    tr = (int(tr[0]), int(tr[1]))\n",
    "    br = (int(br[0]), int(br[1]))\n",
    "    bl = (int(bl[0]), int(bl[1]))\n",
    "    # cleanup the text and draw the box surrounding the text along\n",
    "    # with the OCR text itself\n",
    "    text = cleanup_text(text)\n",
    "    cv2.rectangle(cloneImg, tl, br, (0, 255, 0), 2)\n",
    "#\tcv2.putText(cloneImg, text, (tl[0], tl[1] - 10),\n",
    "#   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "# Uncomment to show image wiht bounding boxes\n",
    "# cv2.imshow(\"cloneImg\", cloneImg)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Print final output text\n",
    "final_text = \"\"\n",
    "for _, text, __ in results:  # _ = bounding box, text = text and __ = confident level\n",
    "    final_text += \" \"\n",
    "    final_text += text\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate image size\n",
    "sz = img.shape\n",
    "\n",
    "# Convert height and width to inches\n",
    "H = 8.5*25.4/0.35\n",
    "W = 11*25.4/0.35\n",
    "\n",
    "# Scale image to PDF size\n",
    "# If height is greater than pdf height scale down\n",
    "if sz[1] > H:\n",
    "    scale1 = H/sz[1]\n",
    "else:\n",
    "    scale1 = 1\n",
    "\n",
    "# If width is greater than pdf width scale down\n",
    "if sz[2] > W:\n",
    "    scale2 = W/sz[2]\n",
    "else:\n",
    "    scale2 = 1\n",
    "\n",
    "# Use larger scale factor\n",
    "if scale1 < scale2:\n",
    "    scale = scale1\n",
    "elif scale2 < scale1:\n",
    "    scale = scale2\n",
    "else:\n",
    "    scale = 1\n",
    "\n",
    "# Store length of results\n",
    "i = len(results)\n",
    "\n",
    "# Define PDF using FPDF\n",
    "pdf = FPDF(orientation='L', unit='pt')\n",
    "pdf.add_page()\n",
    "\n",
    "# Loop through results and save text to PDF\n",
    "for x in range(i):\n",
    "    pdf.set_xy(results[x][0][0][0]*scale, results[x][0][0][1]*scale)\n",
    "    pdf.set_font(\"Arial\", size=16)  # Uncomment for uniform text\n",
    "    # (results[x][0][2][1] - results[x][0][0][1]-3)*scale*0.8)  # Uncomment for scaled text\n",
    "    pdf.cell((results[x][0][1][0] - results[x][0][0][0])*scale, (results[x][0][2][1] - results[x][0][0][1])*scale,\n",
    "             # txt=results[x][1])  # Uncomment for no spell check\n",
    "             txt=spell(results[x][1]))  # Uncomment for spell check\n",
    "\n",
    "# Save output PDF\n",
    "pdf.output(f\"sampleResults/{img_name}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ocrModelRun]",
   "language": "python",
   "name": "conda-env-.conda-ocrModelRun-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
